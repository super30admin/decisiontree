{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# Decision Trees — Student Lab\n",
      "\n",
      "We start using **sklearn** in Week 4, but you’ll still implement core pieces from scratch." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import numpy as np\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 0 — Synthetic dataset\n",
      "We’ll create a non-linear boundary dataset to show how trees fit." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def make_nonlinear(n=400):\n",
      "    X = rng.uniform(-2, 2, size=(n, 2))\n",
      "    # circle boundary\n",
      "    r = np.sqrt(X[:,0]**2 + X[:,1]**2)\n",
      "    y = (r < 1.0).astype(int)\n",
      "    # add noise\n",
      "    flip = rng.random(n) < 0.05\n",
      "    y[flip] = 1 - y[flip]\n",
      "    return X, y\n",
      "\n",
      "X, y = make_nonlinear()\n",
      "n = X.shape[0]\n",
      "idx = rng.permutation(n)\n",
      "tr = idx[: int(0.7*n)]\n",
      "va = idx[int(0.7*n):]\n",
      "Xtr, ytr = X[tr], y[tr]\n",
      "Xva, yva = X[va], y[va]\n",
      "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 1 — Impurity\n",
      "\n",
      "### Task 1.1: Gini impurity\n",
      "\n",
      "# TODO: implement gini(y)\n",
      "# HINT: p_k = count_k / n; gini = 1 - sum(p_k^2)\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def gini(y):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "check('gini_pure0', abs(gini(np.zeros(10, dtype=int))) < 1e-12)\n",
      "check('gini_half', abs(gini(np.array([0,1]*5)) - 0.5) < 1e-12)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 1.2: Entropy\n",
      "\n",
      "# TODO: implement entropy(y)\n",
      "# HINT: entropy = -sum p log2 p (use eps)\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def entropy(y):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "check('entropy_pure0', abs(entropy(np.zeros(10, dtype=int))) < 1e-12)\n",
      "check('entropy_half', abs(entropy(np.array([0,1]*5)) - 1.0) < 1e-9)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 2 — Best split (decision stump)\n",
      "\n",
      "### Task 2.1: Evaluate impurity after threshold split\n",
      "\n",
      "Split rule: go left if X[:,j] <= t else right.\n",
      "Return weighted impurity and information gain.\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def split_indices(X, j, t):\n",
      "    left = np.where(X[:, j] <= t)[0]\n",
      "    right = np.where(X[:, j] > t)[0]\n",
      "    return left, right\n",
      "\n",
      "def info_gain(y, y_left, y_right, criterion='gini'):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "# quick sanity\n",
      "y0 = np.array([0,0,1,1])\n",
      "gain = info_gain(y0, np.array([0,0]), np.array([1,1]), criterion='gini')\n",
      "check('gain_positive', gain > 0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 2.2: Find best (feature, threshold)\n",
      "\n",
      "# TODO: implement best_split(X, y)\n",
      "# HINT: thresholds from sorted unique feature values midpoints\n",
      "\n",
      "**FAANG gotcha:** if a split makes an empty child, skip it." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def best_split(X, y, criterion='gini'):\n",
      "    # TODO: return (best_j, best_t, best_gain)\n",
      "    ...\n",
      "\n",
      "j, t, gain = best_split(Xtr, ytr)\n",
      "print('best', j, t, gain)\n",
      "check('gain_nonneg', gain >= 0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 2.3: Train a stump and evaluate\n",
      "\n",
      "Use best_split to build a stump that predicts majority class on each side.\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def stump_predict(X_train, y_train, X_test, criterion='gini'):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "def accuracy(y, yhat):\n",
      "    return float(np.mean(y == yhat))\n",
      "\n",
      "yhat_tr = stump_predict(Xtr, ytr, Xtr)\n",
      "yhat_va = stump_predict(Xtr, ytr, Xva)\n",
      "print('stump train acc', accuracy(ytr, yhat_tr))\n",
      "print('stump val acc', accuracy(yva, yhat_va))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 3 — sklearn DecisionTreeClassifier (sanity check)\n",
      "\n",
      "### Task 3.1: Train trees with different max_depth\n",
      "\n",
      "# TODO: train sklearn tree and compare train/val accuracy for depth in [1,2,3,5,None].\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "depths = [1,2,3,5,6,7,None]\n",
      "for md in depths:\n",
      "    clf = DecisionTreeClassifier(max_depth=md, random_state=0)\n",
      "    clf.fit(Xtr, ytr)\n",
      "    tr_acc = clf.score(Xtr, ytr)\n",
      "    va_acc = clf.score(Xva, yva)\n",
      "    print('max_depth', md, 'train', tr_acc, 'val', va_acc)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 4 — Failure mode: leakage\n",
      "\n",
      "### Task 4.1: Create a leaky feature\n",
      "Add a feature that is directly derived from y and watch validation accuracy jump.\n",
      "\n",
      "**Explain:** why do trees exploit leakage aggressively?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "Xtr_leak = np.hstack([Xtr, ytr.reshape(-1,1)])\n",
      "Xva_leak = np.hstack([Xva, yva.reshape(-1,1)])\n",
      "\n",
      "clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
      "clf.fit(Xtr_leak, ytr)\n",
      "print('val acc with leakage', clf.score(Xva_leak, yva))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "---\n",
      "## Submission Checklist\n",
      "- All TODOs completed\n",
      "- Stump implemented\n",
      "- sklearn depth sweep shown\n",
      "- Leakage demo explained\n"
    ]}
  ]
}
