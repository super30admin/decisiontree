{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Trees — Student Lab\n",
        "\n",
        "We start using **sklearn** in Week 4, but you’ll still implement core pieces from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic dataset\n",
        "We’ll create a non-linear boundary dataset to show how trees fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n"
          ]
        }
      ],
      "source": [
        "def make_nonlinear(n=400):\n",
        "    X = rng.uniform(-2, 2, size=(n, 2))\n",
        "    # circle boundary\n",
        "    r = np.sqrt(X[:,0]**2 + X[:,1]**2)\n",
        "    y = (r < 1.0).astype(int)\n",
        "    # add noise\n",
        "    flip = rng.random(n) < 0.05\n",
        "    y[flip] = 1 - y[flip]\n",
        "    return X, y\n",
        "\n",
        "X, y = make_nonlinear()\n",
        "n = X.shape[0]\n",
        "idx = rng.permutation(n)\n",
        "tr = idx[: int(0.7*n)]\n",
        "va = idx[int(0.7*n):]\n",
        "Xtr, ytr = X[tr], y[tr]\n",
        "Xva, yva = X[va], y[va]\n",
        "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Impurity\n",
        "\n",
        "### Task 1.1: Gini impurity\n",
        "\n",
        "# TODO: implement gini(y)\n",
        "# HINT: p_k = count_k / n; gini = 1 - sum(p_k^2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: gini_pure0\n",
            "OK: gini_half\n"
          ]
        }
      ],
      "source": [
        "def gini(y):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    if y.size == 0:\n",
        "        return 0.0\n",
        "    p = np.bincount(y) / y.size\n",
        "    return float(1 - np.sum(p**2))\n",
        "\n",
        "check('gini_pure0', abs(gini(np.zeros(10, dtype=int))) < 1e-12)\n",
        "check('gini_half', abs(gini(np.array([0,1]*5)) - 0.5) < 1e-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Entropy\n",
        "\n",
        "# TODO: implement entropy(y)\n",
        "# HINT: entropy = -sum p log2 p (use eps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: entropy_pure0\n",
            "OK: entropy_half\n"
          ]
        }
      ],
      "source": [
        "def entropy(y):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    if y.size == 0:\n",
        "        return 0.0\n",
        "    p1 = y.mean()\n",
        "    p0 = 1 - p1\n",
        "    ent = 0.0\n",
        "    if p0 > 0:\n",
        "        ent -= p0 * np.log2(p0)\n",
        "    if p1 > 0:\n",
        "        ent -= p1 * np.log2(p1)\n",
        "    return float(ent)\n",
        "\n",
        "check('entropy_pure0', abs(entropy(np.zeros(10, dtype=int))) < 1e-12)\n",
        "check('entropy_half', abs(entropy(np.array([0,1]*5)) - 1.0) < 1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Best split (decision stump)\n",
        "\n",
        "### Task 2.1: Evaluate impurity after threshold split\n",
        "\n",
        "Split rule: go left if X[:,j] <= t else right.\n",
        "Return weighted impurity and information gain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: gain_positive\n"
          ]
        }
      ],
      "source": [
        "def split_indices(X, j, t):\n",
        "    left = np.where(X[:, j] <= t)[0]\n",
        "    right = np.where(X[:, j] > t)[0]\n",
        "    return left, right\n",
        "\n",
        "def info_gain(y, y_left, y_right, criterion='gini'):\n",
        "    # TODO\n",
        "    f = gini if criterion == 'gini' else entropy\n",
        "    parent_impurity = f(y)\n",
        "    n = y.size\n",
        "    w1 = (y_left.size / n) * f(y_left) if y_left.size > 0 else 0.0\n",
        "    w2 = (y_right.size / n) * f(y_right) if y_right.size > 0 else 0.0\n",
        "    return parent_impurity - (w1 + w2)\n",
        "\n",
        "# quick sanity\n",
        "y0 = np.array([0,0,1,1])\n",
        "gain = info_gain(y0, np.array([0,0]), np.array([1,1]), criterion='gini')\n",
        "check('gain_positive', gain > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Find best (feature, threshold)\n",
        "\n",
        "# TODO: implement best_split(X, y)\n",
        "# HINT: thresholds from sorted unique feature values midpoints\n",
        "\n",
        "**FAANG gotcha:** if a split makes an empty child, skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best 0 1.026601307828603 0.03378155108197123\n",
            "OK: gain_nonneg\n"
          ]
        }
      ],
      "source": [
        "def best_split(X, y, criterion='gini'):\n",
        "    # TODO: return (best_j, best_t, best_gain)\n",
        "    best = (-1, None,-1.0)\n",
        "    n, d = X.shape\n",
        "    for j in range(d):\n",
        "        vals = np.unique(X[:, j])\n",
        "        if vals.size <2:\n",
        "            continue\n",
        "        thresholds = (vals[:-1] + vals[1:]) / 2\n",
        "        for t in thresholds:\n",
        "            left = X[:, j] <= t\n",
        "            right = ~left\n",
        "            if left.sum() == 0 or right.sum() == 0:\n",
        "                continue\n",
        "            gain = info_gain(y, y[left], y[right], criterion = criterion)\n",
        "            if gain > best[2]:\n",
        "                best = (j, float(t), gain)\n",
        "    return best\n",
        "\n",
        "j, t, gain = best_split(Xtr, ytr)\n",
        "print('best', j, t, gain)\n",
        "check('gain_nonneg', gain >= 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.3: Train a stump and evaluate\n",
        "\n",
        "Use best_split to build a stump that predicts majority class on each side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stump train acc 0.7678571428571429\n",
            "stump val acc 0.75\n"
          ]
        }
      ],
      "source": [
        "def stump_predict(X_train, y_train, X_test, criterion='gini'):\n",
        "    # TODO\n",
        "    j,t ,_ = best_split(X_train, y_train, criterion=criterion)\n",
        "    left = X_train[:, j] <= t\n",
        "    right = ~left\n",
        "    left_label = np.round(y_train[left].mean())\n",
        "    right_label = np.round(y_train[right].mean())\n",
        "    yhat = np.empty(X_test.shape[0], dtype=int)\n",
        "    test_left = X_test[:, j] <= t\n",
        "    yhat[test_left] = left_label\n",
        "    yhat[~test_left] = right_label\n",
        "    return yhat\n",
        "\n",
        "def accuracy(y, yhat):\n",
        "    return float(np.mean(y == yhat))\n",
        "\n",
        "yhat_tr = stump_predict(Xtr, ytr, Xtr)\n",
        "yhat_va = stump_predict(Xtr, ytr, Xva)\n",
        "print('stump train acc', accuracy(ytr, yhat_tr))\n",
        "print('stump val acc', accuracy(yva, yhat_va))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — sklearn DecisionTreeClassifier (sanity check)\n",
        "\n",
        "### Task 3.1: Train trees with different max_depth\n",
        "\n",
        "# TODO: train sklearn tree and compare train/val accuracy for depth in [1,2,3,5,None].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_depth 1 train 0.7678571428571429 val 0.75\n",
            "max_depth 2 train 0.7678571428571429 val 0.75\n",
            "max_depth 3 train 0.8428571428571429 val 0.7666666666666667\n",
            "max_depth 5 train 0.9642857142857143 val 0.8416666666666667\n",
            "max_depth 6 train 0.975 val 0.8333333333333334\n",
            "max_depth 7 train 0.9821428571428571 val 0.825\n",
            "max_depth None train 1.0 val 0.825\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "depths = [1,2,3,5,6,7,None]\n",
        "for md in depths:\n",
        "    clf = DecisionTreeClassifier(max_depth=md, random_state=0)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    tr_acc = clf.score(Xtr, ytr)\n",
        "    va_acc = clf.score(Xva, yva)\n",
        "    print('max_depth', md, 'train', tr_acc, 'val', va_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Failure mode: leakage\n",
        "\n",
        "### Task 4.1: Create a leaky feature\n",
        "Add a feature that is directly derived from y and watch validation accuracy jump.\n",
        "\n",
        "**Explain:** why do trees exploit leakage aggressively?\n",
        "\n",
        "\n",
        "beacuse decision trees choose splits that greedyly reduce impurity and there by creating artificially pure splits. then exploit the ones with strongest signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val acc with leakage 1.0\n"
          ]
        }
      ],
      "source": [
        "Xtr_leak = np.hstack([Xtr, ytr.reshape(-1,1)])\n",
        "Xva_leak = np.hstack([Xva, yva.reshape(-1,1)])\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "clf.fit(Xtr_leak, ytr)\n",
        "print('val acc with leakage', clf.score(Xva_leak, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Stump implemented\n",
        "- sklearn depth sweep shown\n",
        "- Leakage demo explained\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
