{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Trees — Student Lab\n",
        "\n",
        "We start using **sklearn** in Week 4, but you’ll still implement core pieces from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Tree = It's ML algorithm that splits data into branches to make predictions based on feature values.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic dataset\n",
        "We’ll create a non-linear boundary dataset to show how trees fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n"
          ]
        }
      ],
      "source": [
        "# Here creating a data that cannot be separated by straight line\n",
        "def make_nonlinear(n=400):\n",
        "    X = rng.uniform(-2, 2, size=(n, 2))\n",
        "    # circle boundary\n",
        "    r = np.sqrt(X[:,0]**2 + X[:,1]**2) # cretaed a circular boundry\n",
        "    y = (r < 1.0).astype(int) # everything which is less than y , put it in y\n",
        "    # add noise\n",
        "    flip = rng.random(n) < 0.05\n",
        "    y[flip] = 1 - y[flip] # flip the labels of 5% of the data to add noise\n",
        "    return X, y\n",
        "\n",
        "X, y = make_nonlinear()\n",
        "n = X.shape[0]\n",
        "idx = rng.permutation(n) # shuffle the indexes of the data\n",
        "tr = idx[: int(0.7*n)] # take the first 70% of the data for training\n",
        "va = idx[int(0.7*n):] # take the remaining 30% of the data for validation\n",
        "Xtr, ytr = X[tr], y[tr]\n",
        "Xva, yva = X[va], y[va]\n",
        "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Impurity\n",
        "\n",
        "### Task 1.1: Gini impurity\n",
        "\n",
        "# TODO: implement gini(y)\n",
        "# HINT: p_k = count_k / n; gini = 1 - sum(p_k^2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: gini_pure0\n",
            "OK: gini_half\n"
          ]
        }
      ],
      "source": [
        "def gini(y):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    if y.size == 0:\n",
        "        return 0.0\n",
        "    p1 = y.mean()\n",
        "    p0 = 1 - p1\n",
        "    return float(1 -( p0**2 + p1**2)) \n",
        "\n",
        "check('gini_pure0', abs(gini(np.zeros(10, dtype=int))) < 1e-12)\n",
        "check('gini_half', abs(gini(np.array([0,1]*5)) - 0.5) < 1e-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Entropy\n",
        "\n",
        "# TODO: implement entropy(y)\n",
        "# HINT: entropy = -sum p log2 p (use eps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "OK: entropy_pure0\n",
            "OK: entropy_half\n"
          ]
        }
      ],
      "source": [
        "# entropy : measure of uncertainty in a set of labels\n",
        "# ex : how mixed are the labels?\n",
        "# if low entropy, that means most labels are the same\n",
        "# if high entropy, that means labels are evenly mixed\n",
        "# how good the split is? is done by gini or entropy\n",
        "def entropy(y):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    if y.size == 0:\n",
        "        return 0.0\n",
        "    p1 = y.mean() # probability of label 1\n",
        "    p0 = 1 - p1 # probability of label 0\n",
        "    ent = 0.0 # initialize entropy to 0\n",
        "    if p0 > 0: \n",
        "        ent -= p0 * np.log2(p0) # if p0 is greater than 0, then calculate the contribution of label 0 to the entropy\n",
        "    if p1 > 0:\n",
        "        ent -= p1 * np.log2(p1) # if p1 is greater than 0, then calculate the contribution of label 1 to the entropy\n",
        "    return float(ent)\n",
        "\n",
        "print(abs(entropy(np.zeros(10, dtype=int))) < 1e-9)\n",
        "    \n",
        "\n",
        "check('entropy_pure0', abs(entropy(np.zeros(10, dtype=int))) < 1e-12)\n",
        "check('entropy_half', abs(entropy(np.array([0,1]*5)) - 1.0) < 1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Best split (decision stump)\n",
        "\n",
        "### Task 2.1: Evaluate impurity after threshold split\n",
        "\n",
        "Split rule: go left if X[:,j] <= t else right.\n",
        "Return weighted impurity and information gain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: gain_positive\n"
          ]
        }
      ],
      "source": [
        "# Big dataset -> then use gini, small dataset -> use entropy\n",
        "# Gini impurity : if i pick a random label from the set, what is the probability that it is misclassified?\n",
        "\n",
        "def split_indices(X, j, t):\n",
        "    left = np.where(X[:, j] <= t)[0]\n",
        "    right = np.where(X[:, j] > t)[0]\n",
        "    return left, right\n",
        "\n",
        "def info_gain(y, y_left, y_right, criterion='gini'):\n",
        "    # TODO\n",
        "    f = gini if criterion == 'gini' else entropy\n",
        "    parent = f(y) # impurity of the parent node\n",
        "    n = y.size # total number of samples in the parent node\n",
        "    w1 = (y_left.size / n) * f(y_left) # weighted impurity of the left child\n",
        "    w2 = (y_right.size / n) * f(y_right) # weighted impurity of the right child\n",
        "    return float(parent - (w1 + w2)) # information gain is the difference between the impurity of the parent and the weighted impurity of the children\n",
        "\n",
        "# quick sanity\n",
        "y0 = np.array([0,0,1,1])\n",
        "gain = info_gain(y0, np.array([0,0]), np.array([1,1]), criterion='gini')\n",
        "check('gain_positive', gain > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Find best (feature, threshold)\n",
        "\n",
        "# TODO: implement best_split(X, y)\n",
        "# HINT: thresholds from sorted unique feature values midpoints\n",
        "\n",
        "**FAANG gotcha:** if a split makes an empty child, skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best 0 1.026601307828603 0.03378155108197134\n",
            "OK: gain_nonneg\n"
          ]
        }
      ],
      "source": [
        "# Here trying to find the best split for the data\n",
        "def best_split(X, y, criterion='gini'): \n",
        "    # TODO: return (best_j, best_t, best_gain)\n",
        "    best = (-1,None,-1.0) # (feature index, threshold, gain)\n",
        "    n, d = X.shape # number of samples, number of features\n",
        "    for j in range(d):\n",
        "        vals = np.unique(X[:, j]) # unique values of the feature j\n",
        "        if vals.size < 2: # if there are less than 2 unique values, then we cannot split on this feature\n",
        "            continue\n",
        "        thresholds = (vals[:-1] + vals[1:]) / 2 # potential thresholds are the midpoints between unique values\n",
        "        for t in thresholds:\n",
        "            left = X[:, j] <= t # boolean array for left split\n",
        "            right = ~left # boolean array for right split\n",
        "            if left.sum() == 0 or right.sum() == 0: # if either split is empty, then skip this threshold\n",
        "                continue\n",
        "            gain = info_gain(y, y[left], y[right], criterion = criterion) # calculate the information gain for this split\n",
        "            if gain > best[2]:\n",
        "                best = (j, float(t), gain) # update the best split if this gain is better than the best gain so far\n",
        "    return best\n",
        "\n",
        "j, t, gain = best_split(Xtr, ytr)\n",
        "print('best', j, t, gain)\n",
        "check('gain_nonneg', gain >= 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.3: Train a stump and evaluate\n",
        "\n",
        "Use best_split to build a stump that predicts majority class on each side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stump train acc 0.7678571428571429\n",
            "stump val acc 0.75\n"
          ]
        }
      ],
      "source": [
        "def stump_predict(X_train, y_train, X_test, criterion='gini'):\n",
        "    # TODO\n",
        "    j, t, _ = best_split(X_train, y_train, criterion=criterion) # find the best split on the training data\n",
        "    left = X_train[:, j] <= t # boolean array for left split\n",
        "    right = ~left # boolean array for right split\n",
        "    left_label = int(np.round(y_train[left].mean()) )# majority label for left split , how many 1s are there in the left split, if more than 0.5, then label is 1, otherwise label is 0\n",
        "    right_label = int(np.round(y_train[right].mean()) )# majority label for right split\n",
        "    test_left = X_test[:, j] <= t # boolean array for test samples that fall into the left split\n",
        "    yhat = np.empty(X_test.shape[0], dtype=int) # initialize predictions to\n",
        "    yhat[test_left] = left_label # assign the left label to the test samples that fall into the left split\n",
        "    yhat[~test_left] = right_label # assign the right label to the test samples that fall into the right split\n",
        "    return yhat\n",
        "\n",
        "# accuracy : measure of how many predictions are correct\n",
        "def accuracy(y, yhat):\n",
        "    return float(np.mean(y == yhat))\n",
        "\n",
        "yhat_tr = stump_predict(Xtr, ytr, Xtr)\n",
        "yhat_va = stump_predict(Xtr, ytr, Xva)\n",
        "print('stump train acc', accuracy(ytr, yhat_tr))\n",
        "print('stump val acc', accuracy(yva, yhat_va))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — sklearn DecisionTreeClassifier (sanity check)\n",
        "\n",
        "### Task 3.1: Train trees with different max_depth\n",
        "\n",
        "# TODO: train sklearn tree and compare train/val accuracy for depth in [1,2,3,5,None].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_depth 1 train 0.7678571428571429 val 0.75\n",
            "max_depth 2 train 0.7678571428571429 val 0.75\n",
            "max_depth 3 train 0.8428571428571429 val 0.7666666666666667\n",
            "max_depth 5 train 0.9642857142857143 val 0.8416666666666667\n",
            "max_depth None train 1.0 val 0.825\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "depths = [1,2,3,5,None]\n",
        "for md in depths:\n",
        "    clf = DecisionTreeClassifier(max_depth=md, random_state=0) \n",
        "    clf.fit(Xtr, ytr)\n",
        "    tr_acc = clf.score(Xtr, ytr)\n",
        "    va_acc = clf.score(Xva, yva)\n",
        "    print('max_depth', md, 'train', tr_acc, 'val', va_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Failure mode: leakage\n",
        "\n",
        "### Task 4.1: Create a leaky feature\n",
        "Add a feature that is directly derived from y and watch validation accuracy jump.\n",
        "\n",
        "**Explain:** why do trees exploit leakage aggressively?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val acc with leakage 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "Xtr_leak = np.hstack([Xtr, ytr.reshape(-1,1)])\n",
        "Xva_leak = np.hstack([Xva, yva.reshape(-1,1)])\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "clf.fit(Xtr_leak, ytr)\n",
        "print('val acc with leakage', clf.score(Xva_leak, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Stump implemented\n",
        "- sklearn depth sweep shown\n",
        "- Leakage demo explained\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
